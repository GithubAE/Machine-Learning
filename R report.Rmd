---
title: "CourseProject"
output: html_document
---



Cleaning
-----------------------------------------------------

First of all the the training data was cleaned of NAs. Where more than 90% NAs existed or where 90% of the nominal columns displayed the same value, the column was excluded.

```{r}

#input=unprocessed data set
#output = preprocessed data set (removel of )
#-------------------------------------------------------------------------------------------------
#-------------------------------------------------------------------------------------------------

DataPreprocessing <- function(data){
        
        dataA<-isNa(data)    
        dataB<-DivReplacement(dataA) # replaces Div/ Errors with NA values
        dataC<-ExcludeMissing(dataB) # calling the subfunction NA proportion to decide which data to keep and which to exclude
       
   
        write.csv(dataC, file = "E://University/Coursera/Machine Learning/preprocessed.csv")
        return(dataC)
        
}

#-------------------------------------------------------------------------------------------------
#-------------------------------------------------------------------------------------------------





#-------------------------------------------------------------------------------------------------------
#Columns to exclude
#input = Data Set
#output = vector NaProportions
NaProportion<-function(data){
        
  
        cols<-ncol(data) #number of cols
        Exclude<- vector(mode = "numeric", length = cols)
                
                
        for (i in 1:cols) #goes through all columns
        {
                
                
                                       
                        
                        column<-(data[,i])
                        nacount<-0
                        
                        for (x in 1:length(data[,i])) {   #calculates the na rate per column
                                
                                
                                if (is.na(column[x])) {nacount<-nacount + 1} #checks for nas and increments nacoutn
                        
                       
                        
                        } 
                        
                        
                        NaRate<-nacount/length(data[,i]) #na rate as a percentage
                        is.numeric
                        Exclude[i]<-NaRate  #saves the na rate in a vector
                 
                
                
        } 
        
      return(Exclude)  #returns the na rate as vector
        
        
}








#-------------------------------------------------------------------------------------------------------
#input data frame
#output data frame with NAs declared in "" places
isNa<-function(data){
        
        
        cols<-ncol(data) #number of cols
        
        
        for (i in 1:cols) #goes through all columns
        {
                
                
                
                
                column<-(data[,i])
                
                for (x in 1:length(column)) {   #calculates the na rate per column
                        
                        
                        if (column[x] == "" && !is.na(column[x])) {column[x]<-NA} #checks for nas and increments nacoutn
                         
                        
                } 
                
                data[,i]<-column
                
                
                
        } 
        
        return(data)  #returns the na rate as vector
        
        
}



#-------------------------------------------------------------------------------------------------------
#input data frame
#output data frame with NAs declared in "#DIV/0!" places
DivReplacement<-function(data){
        
        
        cols<-ncol(data) #number of cols
        
        
        for (i in 1:cols) #goes through all columns
        {
                
                
                
                
                column<-(data[,i])
                
                for (x in 1:length(column)) {   #calculates the na rate per column
                        
                        
                        if (column[x] == "#DIV/0!" && !is.na(column[x])) {column[x]<-NA} #checks for nas and increments nacoutn
                        
                        
                } 
                
                data[,i]<-column
                
                
                
        } 
        
        return(data)  #returns the na rate as vector
        
        
}





#-------------------------------------------------------------------------------------------------------

#Input = data frame
#returns data frame without missing values
ExcludeMissing<-function(data){
        
        
        Nas<-NaProportion(data)
        
        Nas<-Nas > 0.9
      
        
        
        data<-data[, Nas==FALSE]

return(data)


}






#-------------------------------------------------------------------------------------------------------



```



In a subsequent step I used RapidMiner to remove correlated attributes, using a treshold of 0.9 Correlation.This reduced the number of columns in the training data set to below 60. After that, after careful examination of the training data set, the variables "att1" and "user" were removed. The reason for this removal was that they couldnt have been possibly relevant in determining the nature of a training excercise. Graphical examination showed that exercise outcomes "classe" where equally distributed across users. The final number of columns included was 53. 




Training
-----------------------------------------------------
----------------------------------------------------------------------------------
I framed the training process as a function, which one has to pass the data set to trained. According to preference, one can the function have return either a confusion matrix or the model, depending on which piece of code is being #commented out.

I adhered to the 60 40 split of training to testing set. The data was divided into k 10 folds, which implies ten cross validation steps. In each step one withheld data set is being withheld. This choice of 10 times cross validation is needed to get a better approximation of the out of sample error and to account for in sample biases. Hence, in sample bias and therefore in sample error rate is reduced. 

Here the function is configured in a way that it returns the model which can then be stored in a variable. 


```{r}

training<-function(data){
        
        
        trainIndex <- createDataPartition(data$classe, p = .6,
                                          list = FALSE,
                                          times = 1)        
        
        ctrl<-trainControl(method ="cv", number = 10, p= .6 )
        
        train<-data[trainIndex,]
        test<-data[-trainIndex,]
        

         modelFit<-train(classe ~ ., method="rf", data = train, trControl = ctrl) #build the model using a decision tree
        
        predictions <- predict(modelFit, data[,-ncol(data)]) #make prediction, "modelFit" is the 
        #ConfMatrix<-confusionMatrix(predictions, data$classe) #compares the predictions to the real classe attribute in the data set if needed
                                                                #can be used when required
        return(modelFit) # returns model or on demand confusion matrix
        
}


```




Model
-----------------------------------------------------
----------------------------------------------------------------------------------

The model is the following:
         
    `     Reference
Prediction    A    B    C    D    E
         A 5471    2    0    0    0
         B    0 3716    1    0    0
         C    0    0 3351    2    0
         D    0    0    0 3145    0
         E    0    0    0    0 3528

`


Out of sample error:

The inner sample error can be determined to be at 1-accuracy. The reason for this is that we compare the model results against the actual results of the traing set in the confusion matrix


The out of sample error rate cannot be estimated with confidence, since the sample might have a bias. The fact that I use k folds, reduces the variation in the estimation error, as the evaluation set is changed 10 times, and the model is therefore build ten times with different trianing data. This flattens out possible skewness/biases for the inner sample error rate and gives a rough estimeate of the out of smample error.

According to the above error rate, the out of sample rate could be 1- 0.99, while it is more likely that it is a bit higher than the estimate from cross validation.



```{r}

#Random Forest 


#11532 samples
 #  50 predictor
  #  5 classes: 'A', 'B', 'C', 'D', 'E' 

#No pre-processing

#Resampling: Cross-Validated (10 fold) 

#Summary of sample sizes: 10380, 10378, 10380, 10379, 10380, 10379, ... 

#Resampling results across tuning parameters:

 # mtry  Accuracy   Kappa      Accuracy SD   Kappa SD   
  # 2    0.9882068  0.9850774  0.0034250992  0.004335511
 # 37    0.9986992  0.9983545  0.0008426091  0.001065908
  #72    0.9982657  0.9978061  0.0009137422  0.001156010

#Accuracy was used to select the optimal model using  the largest value.
#The final value used for the model was mtry = 37. 

```





When applying the rf model to the unseen test data set, the following is predicted. 



 Prediction
 [1] B A B A A E D B A A B C B A E E A B B B

All of these predictions turned out to be true upon submission. This indicates that the out of smaple error is indeed quite low. 
